import streamlit as st
from streamlit_option_menu import option_menu
from PIL import Image
import pickle
import pandas as pd
import scipy.sparse as sp

from ultils import content_process, helper, product_analysis

st.set_page_config(page_title="Sentiment Analysis System", page_icon=":shopping_cart:", layout="wide")

menu = ["Project Summary", "Sentiment Analysis", "Product Analysis", ]


# Banner Image
image = Image.open("src/images/hasaki_banner.jpg")
st.image(image)

button_style = """
    <style>
    .stButton>button {
        background-color: #326e51;
        color: white;
        border: none;
        padding: 10px 24px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 16px;`
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 12px;
    }
    </style>
    """
st.sidebar.title("ƒê·ªì √Ån T·ªët Nghi·ªáp - K299")
st.sidebar.markdown(
    """
    <h2 style="display: flex; align-items: center; font-size: 22px;">
        <span style="margin-left: 8px;">Sentiment Analysis Project</span>
    </h2>
    """,
    unsafe_allow_html=True,
)

# sidebar menu
with st.sidebar:
    page = option_menu("Ch·ª©c NƒÉng Ch√≠nh", menu, 
        icons=['house', 'cloud-upload', "list-task", 'gear'], 
        menu_icon="cast", 
        styles={
            "container": {"padding": "0!important", "background-color": "#e9ecef"},
            "icon": {"color": "#326e51", "font-size": "1.1rem"}, 
            "nav-link": {"font-size": "18px", "text-align": "left", "margin":"0px", "--hover-color": "#eee", "font-weight": "500"},
            "nav-link-selected": {"background-color": "#ebf8ee", "color": "#326e51"},
            "menu-title": { "font-size": "1.1rem", "font-weight": "600", "font-family": "Source Sans Pro, sans-serif"},
        },
        default_index=0)
st.sidebar.markdown(
    """
    <style>
        .footer {
            text-align: center;
            font-size: 12px;
        }
        hr {
            border: 0.5px solid gray;
        }
    </style>
    <div class="footer">
        <hr>
    </div>
    """,
    unsafe_allow_html=True,
)

st.sidebar.markdown(
    """
    <h3 style="display: flex; align-items: center; font-size: 18px;">
        <span style="margin-left: 8px;">Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n</span>
    </h3>
    """,
    unsafe_allow_html=True,
)
st.sidebar.markdown("üë©‚Äçüè´ [Th·∫°c Sƒ© Khu·∫•t Th√πy Ph∆∞∆°ng](https://csc.edu.vn/giao-vien~37#)")

# Subheader with an icon for "Th√†nh Vi√™n"
st.sidebar.markdown(
    """
    <style>
        .footer {
            text-align: center;
            font-size: 12px;
        }
        hr {
            border: 0.5px solid gray;
        }
    </style>
    <div class="footer">
        <hr>
    </div>
    """,
    unsafe_allow_html=True,
)
st.sidebar.markdown(
    """
    <h3 style="display: flex; align-items: center; font-size: 18px;">
        <span style="margin-left: 8px;">Th√†nh Vi√™n</span>
    </h3>
    """,
    unsafe_allow_html=True,
)
st.sidebar.markdown(" :boy: M√£ Th·∫ø Nh·ª±t")
st.sidebar.markdown(" :girl: T·ª´ Th·ªã Thanh Xu√¢n")

# Subheader with an icon for "Ng√†y b√°o c√°o t·ªët nghi·ªáp"
st.sidebar.markdown(
    """
    <style>
        .footer {
            text-align: center;
            font-size: 12px;
        }
        hr {
            border: 0.5px solid gray;
        }
    </style>
    <div class="footer">
        <hr>
    </div>
    """,
    unsafe_allow_html=True,
)

st.sidebar.markdown(
    """
    <h3 style="display: flex; align-items: center; font-size: 18px;">
        <span style="margin-left: 8px;">Ng√†y b√°o c√°o t·ªët nghi·ªáp:</span>
    </h3>
    """,
    unsafe_allow_html=True,
)
st.sidebar.write('üìÖ 16/12/2024')

# Add spacer for footer positioning
st.sidebar.markdown("<div style='height: 100px;'></div>", unsafe_allow_html=True)

# Sidebar footer
st.sidebar.write("¬© 2024 Hasaki Sentiment Analysis System")


# Main page logic
if page == "Project Summary":
    
    tab_containers = st.tabs(['Hasaki Project', 'Th·ª±c Hi·ªán D·ª± √Ån'])

    with tab_containers[0]:
        # Title Section
        st.title("D·ª± √°n HASAKI: Ph√¢n t√≠ch ph·∫£n h·ªìi kh√°ch h√†ng üõçÔ∏è")

        # Introduction Section
        st.subheader("1. T·ªïng quan v·ªÅ HASAKI")
        st.markdown("""
        **HASAKI.VN** l√† h·ªá th·ªëng c·ª≠a h√†ng m·ªπ ph·∫©m ch√≠nh h√£ng v√† d·ªãch v·ª• chƒÉm s√≥c s·∫Øc ƒë·∫πp chuy√™n s√¢u, v·ªõi:  
        - üõí **H·ªá th·ªëng c·ª≠a h√†ng tr·∫£i d√†i tr√™n to√†n qu·ªëc.**  
        - ü§ù **L√† ƒë·ªëi t√°c ph√¢n ph·ªëi chi·∫øn l∆∞·ª£c t·∫°i Vi·ªát Nam** c·ªßa h√†ng lo·∫°t th∆∞∆°ng hi·ªáu m·ªπ ph·∫©m l·ªõn.  
        - üåê **N·ªÅn t·∫£ng tr·ª±c tuy·∫øn** gi√∫p kh√°ch h√†ng d·ªÖ d√†ng:
            - L·ª±a ch·ªçn s·∫£n ph·∫©m.  
            - Xem ƒë√°nh gi√°/nh·∫≠n x√©t th·ª±c t·∫ø.  
            - ƒê·∫∑t mua h√†ng nhanh ch√≥ng.  
        """)

        # Problem Section
        st.subheader("2. V·∫•n ƒë·ªÅ ƒë·∫∑t ra üö©")
        st.markdown("""
        üßê **C√¢u h·ªèi ƒë·∫∑t ra:**  
        - L√†m th·∫ø n√†o ƒë·ªÉ **c√°c nh√£n h√†ng hi·ªÉu r√µ h∆°n v·ªÅ kh√°ch h√†ng**?  
        - L√†m sao ƒë·ªÉ bi·∫øt **kh√°ch h√†ng ƒë√°nh gi√° g√¨ v·ªÅ s·∫£n ph·∫©m**?  
        - L√†m c√°ch n√†o ƒë·ªÉ t·ª´ ph·∫£n h·ªìi ƒë√≥, c√°c nh√£n h√†ng c√≥ th·ªÉ:  
            - **C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng s·∫£n ph·∫©m.**  
            - **N√¢ng c·∫•p d·ªãch v·ª• ƒëi k√®m.**  

        **Hasaki.vn s·ªü h·ªØu l∆∞·ª£ng l·ªõn d·ªØ li·ªáu t·ª´ c√°c b√¨nh lu·∫≠n v√† ƒë√°nh gi√° c·ªßa kh√°ch h√†ng, tuy nhi√™n:**
        - üö´ **Kh√¥ng th·ªÉ x·ª≠ l√Ω ph·∫£n h·ªìi nhanh ch√≥ng v√† ch√≠nh x√°c.**  
        - ü§î **Kh√≥ x√°c ƒë·ªãnh ph·∫£n h·ªìi t√≠ch c·ª±c, ti√™u c·ª±c hay trung t√≠nh.**  
        - üïí **T·ªën th·ªùi gian ƒë·ªÉ s·ª≠ d·ª•ng ph·∫£n h·ªìi cho vi·ªác c·∫£i thi·ªán s·∫£n ph·∫©m/d·ªãch v·ª•.**
        """)

        # Objective Section
        st.subheader("3. M·ª•c ti√™u üéØ")
        st.markdown("""
        üõ†Ô∏è **X√¢y d·ª±ng h·ªá th·ªëng d·ª± ƒëo√°n c·∫£m x√∫c trong ph·∫£n h·ªìi kh√°ch h√†ng**:
        1. **Ph√¢n lo·∫°i c√°c b√¨nh lu·∫≠n th√†nh 2 lo·∫°i**: T√≠ch C·ª±c, Ti√™u C·ª±c.
        2. **ƒê√°nh gi√° chi ti·∫øt t·ª´ng s·∫£n ph·∫©m d·ª±a v√†o c√°c b√¨nh lu·∫≠n.** 
        3. **Gi√∫p Hasaki v√† ƒë·ªëi t√°c**:
            - Hi·ªÉu nhanh √Ω ki·∫øn kh√°ch h√†ng.
            - **C·∫£i thi·ªán s·∫£n ph·∫©m/d·ªãch v·ª•** d·ª±a tr√™n ph·∫£n h·ªìi th·ª±c t·∫ø.
            - **TƒÉng m·ª©c ƒë·ªô h√†i l√≤ng v√† trung th√†nh** c·ªßa kh√°ch h√†ng.
        """)

        # Expected Outcomes Section
        st.subheader("4. K·∫øt qu·∫£ k·ª≥ v·ªçng ‚úÖ")
        st.markdown("""
        - **Ch√≠nh x√°c ‚â•90%** trong ph√¢n lo·∫°i ph·∫£n h·ªìi kh√°ch h√†ng.  
        - **Cung c·∫•p ph√¢n t√≠ch th·ªùi gian th·ª±c** cho Hasaki v√† ƒë·ªëi t√°c.  
        - TƒÉng s·ª± **h√†i l√≤ng** v√† **l√≤ng trung th√†nh** c·ªßa kh√°ch h√†ng th√¥ng qua c√°c c·∫£i ti·∫øn.
        """)

    with tab_containers[1]:  # Assuming the second tab is for the project process
        st.subheader("Quy tr√¨nh th·ª±c hi·ªán üí°")

        # Step 1: Data Collection
        st.subheader("1. Thu th·∫≠p d·ªØ li·ªáu üìù")
        st.markdown("""
        - **Ngu·ªìn d·ªØ li·ªáu:** B√¨nh lu·∫≠n v√† ƒë√°nh gi√° t·ª´ kh√°ch h√†ng tr√™n Hasaki.vn.  
        - **M·ª•c ti√™u:** T·∫°o t·∫≠p d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng cao ƒë·ªÉ ƒë√†o t·∫°o v√† ƒë√°nh gi√° m√¥ h√¨nh h·ªçc m√°y.
        """)
        image = Image.open("src/images/rv2.png")
        st.image(image, caption="ƒê√°nh gi√° v·ªÅ s·∫£n ph·∫©m n∆∞·ªõc Hoa H·ªìng Klairs Kh√¥ng M√πi Cho Da Nh·∫°y C·∫£m 180ml Supple Preparation Unscented Toner")
    
        # Step 2: Data Processing Section
        st.subheader("2. X·ª≠ l√Ω d·ªØ li·ªáu üîÑ")
        st.markdown("""
        ### M·ª•c ti√™u:
        Chu·∫©n b·ªã v√† l√†m s·∫°ch d·ªØ li·ªáu b√¨nh lu·∫≠n ƒë·ªÉ s·∫µn s√†ng cho c√°c b∆∞·ªõc ph√¢n t√≠ch v√† x√¢y d·ª±ng m√¥ h√¨nh.

        ### Quy tr√¨nh x·ª≠ l√Ω:
        1. **L√†m s·∫°ch d·ªØ li·ªáu:**
        - Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (v√≠ d·ª•: `!`, `@`, `#`, ...).
        - X√≥a s·ªë v√† c√°c t·ª´ kh√¥ng mang √Ω nghƒ©a ng·ªØ nghƒ©a (stop words).
        2. **Chu·∫©n h√≥a vƒÉn b·∫£n:**
        - Chuy·ªÉn to√†n b·ªô vƒÉn b·∫£n v·ªÅ ch·ªØ th∆∞·ªùng.
        - Chu·∫©n h√≥a k√Ω t·ª± l·∫∑p l·∫°i (v√≠ d·ª•: `"ƒë·∫πp qu√°aaa"` ‚Üí `"ƒë·∫πp qu√°"`).
        - S·ª≠ d·ª•ng c√¥ng c·ª• NLP ƒë·ªÉ t√°ch t·ª´ v√† chu·∫©n b·ªã d·ªØ li·ªáu.
        3. **Chuy·ªÉn ƒë·ªïi ng√¥n ng·ªØ:**
        - Thay th·∫ø c√°c t·ª´ teencode th√†nh d·∫°ng chu·∫©n (v√≠ d·ª•: `"hok"` ‚Üí `"kh√¥ng"`).
        - D·ªãch t·ª´ ti·∫øng Anh sang ti·∫øng Vi·ªát (n·∫øu c√≥).
        4. **X·ª≠ l√Ω ng·ªØ ph√°p (POS Tagging):**
        - Ph√¢n t√≠ch t·ª´ lo·∫°i v√† g√°n nh√£n t·ª´ v·ª±ng ƒë·ªÉ hi·ªÉu c·∫•u tr√∫c c√¢u.
        5. **T·∫°o ph√¢n ƒëo·∫°n (Chunking):**
        - Chia nh·ªè vƒÉn b·∫£n th√†nh c√°c kh·ªëi th√¥ng tin c√≥ √Ω nghƒ©a ƒë·ªÉ d·ªÖ d√†ng x·ª≠ l√Ω v√† ph√¢n t√≠ch.

        ### K·∫øt qu·∫£ k·ª≥ v·ªçng:
        - D·ªØ li·ªáu s·∫°ch v√† chu·∫©n h√≥a, l∆∞u tr·ªØ d∆∞·ªõi d·∫°ng c·ªôt vƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω v√† ph√¢n ƒëo·∫°n.
        """)
        image = Image.open("src/images/output1.png")
        st.image(image, caption="N·ªôi dung b√¨nh lu·∫≠n tr∆∞·ªõc v√† sau khi x·ª≠ l√Ω")
        
        # Step 3: Labeling and Sentiment Analysis Section
        st.subheader("3. G·∫Øn nh√£n v√† ph√¢n t√≠ch c·∫£m x√∫c üè∑Ô∏èüìä")
        st.markdown("""
        ### M·ª•c ti√™u:
        - G·∫Øn nh√£n (label) cho d·ªØ li·ªáu b√¨nh lu·∫≠n ƒë·ªÉ s·ª≠ d·ª•ng trong ph√¢n t√≠ch c·∫£m x√∫c ho·∫∑c c√°c m√¥ h√¨nh h·ªçc m√°y gi√°m s√°t.
        - Ph√¢n t√≠ch s·ªë l∆∞·ª£ng ƒë√°nh gi√° theo s·∫£n ph·∫©m v√† c·∫£m x√∫c (t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c), ƒë·ªìng th·ªùi t√≠nh t·ª∑ l·ªá c·∫£m x√∫c ƒë·ªÉ hi·ªÉu r√µ ph·∫£n h·ªìi c·ªßa kh√°ch h√†ng.

        ### Quy tr√¨nh th·ª±c hi·ªán:
        1. **G·∫Øn nh√£n b√¨nh lu·∫≠n:**
        - **X√°c ƒë·ªãnh t·ª´ t√≠ch c·ª±c v√† ti√™u c·ª±c:** T√¨m v√† ƒë·∫øm c√°c t·ª´ t√≠ch c·ª±c (v√≠ d·ª•: `t·ªët`, `ƒë·∫πp`, `th√≠ch`) v√† ti√™u c·ª±c (v√≠ d·ª•: `x·∫•u`, `t·ªá`, `th·∫•t v·ªçng`) trong b√¨nh lu·∫≠n.
        - **T√≠nh to√°n t·ª∑ l·ªá t·ª´:**
            - **T·ª∑ l·ªá t·ª´ t√≠ch c·ª±c:** T·ªïng s·ªë t·ª´ t√≠ch c·ª±c chia cho t·ªïng s·ªë t·ª´ trong b√¨nh lu·∫≠n.
            - **T·ª∑ l·ªá t·ª´ ti√™u c·ª±c:** T·ªïng s·ªë t·ª´ ti√™u c·ª±c chia cho t·ªïng s·ªë t·ª´ trong b√¨nh lu·∫≠n.
        - **Ph√¢n lo·∫°i b√¨nh lu·∫≠n:**
            - **T√≠ch c·ª±c:** Khi t·ª∑ l·ªá t·ª´ t√≠ch c·ª±c cao h∆°n ti√™u c·ª±c v√† >= 2.
            - **Ti√™u c·ª±c:** C√°c tr∆∞·ªùng h·ª£p c√≤n l·∫°i.

        2. **Ph√¢n t√≠ch c·∫£m x√∫c theo s·∫£n ph·∫©m:**
        - **T·ªïng h·ª£p ƒë√°nh gi√° theo s·∫£n ph·∫©m v√† c·∫£m x√∫c:**
            - ƒê·∫øm s·ªë l∆∞·ª£ng ƒë√°nh gi√° t√≠ch c·ª±c v√† ti√™u c·ª±c theo t·ª´ng s·∫£n ph·∫©m.
            - G·ªôp t·∫•t c·∫£ c√°c b√¨nh lu·∫≠n, t·ª´ t√≠ch c·ª±c, v√† t·ª´ ti√™u c·ª±c th√†nh chu·ªói vƒÉn b·∫£n.
        - **T√≠nh t·ªïng s·ªë l∆∞·ª£ng ƒë√°nh gi√° m·ªói s·∫£n ph·∫©m:** T·ªïng s·ªë l∆∞·ª£ng ƒë√°nh gi√° t·ª´ t·∫•t c·∫£ c√°c c·∫£m x√∫c cho t·ª´ng s·∫£n ph·∫©m.
        - **T√≠nh t·ª∑ l·ªá c·∫£m x√∫c:**
            - **C√¥ng th·ª©c:** `(S·ªë l∆∞·ª£ng ƒë√°nh gi√° theo c·∫£m x√∫c / T·ªïng s·ªë ƒë√°nh gi√°) * 100`.
            - K·∫øt qu·∫£ ƒë∆∞·ª£c l∆∞u trong c·ªôt `Sentiment_Rate (%)`.

        ### K·∫øt qu·∫£ k·ª≥ v·ªçng:
        - D·ªØ li·ªáu b√¨nh lu·∫≠n ƒë∆∞·ª£c g·∫Øn nh√£n:
        - `1` cho b√¨nh lu·∫≠n **t√≠ch c·ª±c**.
        - `0` cho b√¨nh lu·∫≠n **ti√™u c·ª±c**.
        - Th·ªëng k√™ s·ªë l∆∞·ª£ng ƒë√°nh gi√° t√≠ch c·ª±c, ti√™u c·ª±c, v√† t·ªïng s·ªë ƒë√°nh gi√° theo t·ª´ng s·∫£n ph·∫©m.
        - T√≠nh t·ª∑ l·ªá ph·∫ßn trƒÉm c·∫£m x√∫c t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c cho m·ªói s·∫£n ph·∫©m, h·ªó tr·ª£ ph√¢n t√≠ch xu h∆∞·ªõng c·∫£m x√∫c c·ªßa kh√°ch h√†ng.
        """)
        image = Image.open("src/images/label.png")
        st.image(image, caption="D·ªØ li·ªáu sau khi ƒë∆∞·ª£c label")
        
        # Step 4: Detailed Product Analysis Section
        st.subheader("4. Ph√¢n t√≠ch chi ti·∫øt s·∫£n ph·∫©m üõçÔ∏è")
        st.markdown("""
        ### M·ª•c ti√™u:
        Cung c·∫•p th√¥ng tin chi ti·∫øt v·ªÅ s·∫£n ph·∫©m, bao g·ªìm:
        - Th√¥ng tin c∆° b·∫£n (t√™n s·∫£n ph·∫©m, gi√° b√°n, m√¥ t·∫£, ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh).
        - Ph√¢n t√≠ch ƒë√°nh gi√° c·∫£m x√∫c (t√≠ch c·ª±c v√† ti√™u c·ª±c).
        - H√¨nh dung d·ªØ li·ªáu th√¥ng qua bi·ªÉu ƒë·ªì v√† Word Cloud.

        ### Quy tr√¨nh:
        1. **Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n v·ªÅ s·∫£n ph·∫©m**:
        - T√™n s·∫£n ph·∫©m, gi√° b√°n, gi√° g·ªëc, ph√¢n lo·∫°i, m√¥ t·∫£, ƒëi·ªÉm ƒë√°nh gi√° trung b√¨nh.
        2. **Ph√¢n t√≠ch ƒë√°nh gi√° c·∫£m x√∫c**:
        - Hi·ªÉn th·ªã t·ª∑ l·ªá c·∫£m x√∫c t√≠ch c·ª±c v√† ti√™u c·ª±c b·∫±ng bi·ªÉu ƒë·ªì h√¨nh tr√≤n.
        - T·∫°o Word Cloud cho c√°c t·ª´ t√≠ch c·ª±c v√† ti√™u c·ª±c.
        3. **Ki·ªÉm tra d·ªØ li·ªáu**:
        - N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, th√¥ng b√°o s·∫Ω ƒë∆∞·ª£c hi·ªÉn th·ªã ƒë·ªÉ ng∆∞·ªùi d√πng bi·∫øt.
        """)
        image = Image.open("src/images/info.png")
        st.image(image)
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            image = Image.open("src/images/pie.png")
            st.image(image)

        # Center the title using HTML
        st.markdown(
            """
            <h3 style="text-align: center;">Word Cloud - Positive and Negative Sentiment:</h3>
            """,
            unsafe_allow_html=True
        )

        # Display Positive and Negative Word Cloud Images Side-by-Side
        cols = st.columns(2)

        # Positive Word Cloud
        with cols[0]:
            image = Image.open("src/images/pos.png")
            st.image(image)

        # Negative Word Cloud
        with cols[1]:
            image = Image.open("src/images/neg.png")
            st.image(image)

        # Step 5: Data Preparation for Machine Learning
        st.subheader("5. Chu·∫©n b·ªã d·ªØ li·ªáu cho M√¥ h√¨nh H·ªçc m√°y ü§ñ")

        st.markdown("""
        ### M·ª•c ti√™u:
        Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫ßu v√†o ch·∫•t l∆∞·ª£ng cao cho c√°c m√¥ h√¨nh h·ªçc m√°y b·∫±ng c√°ch:
        - Bi·∫øn ƒë·ªïi vƒÉn b·∫£n th√†nh ƒë·∫∑c tr∆∞ng s·ªë s·ª≠ d·ª•ng TF-IDF.
        - B·ªï sung c√°c ƒë·∫∑c tr∆∞ng h·ªó tr·ª£ ph√¢n t√≠ch (ƒë·ªô d√†i ƒë√°nh gi√°).
        - X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh.

        ### Quy tr√¨nh:
        1. **Ph√¢n chia d·ªØ li·ªáu:**
            - T√°ch d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán (80%) v√† t·∫≠p ki·ªÉm tra (20%).
        2. **Bi·∫øn ƒë·ªïi vƒÉn b·∫£n b·∫±ng TF-IDF:**
            - S·ª≠ d·ª•ng `TfidfVectorizer` ƒë·ªÉ t·∫°o ƒë·∫∑c tr∆∞ng t·ª´ vƒÉn b·∫£n (gi·ªØ l·∫°i 5000 t·ª´ quan tr·ªçng nh·∫•t).
        3. **Th√™m ƒë·∫∑c tr∆∞ng h·ªó tr·ª£:**
            - T√≠nh ƒë·ªô d√†i vƒÉn b·∫£n v√† chu·∫©n h√≥a b·∫±ng `MinMaxScaler`.
        4. **K·∫øt h·ª£p ƒë·∫∑c tr∆∞ng:**
            - K·∫øt h·ª£p TF-IDF v√† ƒë·∫∑c tr∆∞ng ƒë·ªô d√†i th√†nh m·ªôt ma tr·∫≠n ƒë·∫ßu v√†o duy nh·∫•t.
        5. **X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp:**
            - √Åp d·ª•ng SMOTE ƒë·ªÉ t·∫°o th√™m d·ªØ li·ªáu cho l·ªõp nh·ªè, ƒë·∫£m b·∫£o d·ªØ li·ªáu c√¢n b·∫±ng.
        """)

        # Step 6: Evaluation and Improvement
        st.subheader("6. ƒê√°nh gi√° v√† c·∫£i thi·ªán üìä")

        st.markdown("""
        ### Ti√™u ch√≠ ƒë√°nh gi√°:
        1. **ƒê·ªô ch√≠nh x√°c (Accuracy):** X√°c ƒë·ªãnh t·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng.
        2. **F1-Score:** ƒêo l∆∞·ªùng c√¢n b·∫±ng gi·ªØa Precision v√† Recall.
        3. **ROC-AUC:** Hi·ªáu qu·∫£ ph√¢n lo·∫°i d·ª±a tr√™n ƒë∆∞·ªùng cong ROC.

        ### Quy tr√¨nh c·∫£i thi·ªán:
        - D·ª±a tr√™n c√°c ch·ªâ s·ªë v√† ph·∫£n h·ªìi t·ª´ ng∆∞·ªùi d√πng h·ªá th·ªëng.
        - Th·ª≠ nghi·ªám th√™m c√°c m√¥ h√¨nh ho·∫∑c ƒë·∫∑c tr∆∞ng b·ªï sung.
        """)
        # Step 7: Model Comparison and Recommendation
        st.subheader("7. So s√°nh v√† l·ª±a ch·ªçn m√¥ h√¨nh t·ªëi ∆∞u üìä")
        st.markdown("""
        ### M·ª•c ti√™u:
        So s√°nh hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh Logistic Regression, Multinomial Naive Bayes, XGBoost, v√† SVM.
        ƒê∆∞a ra l·ª±a ch·ªçn m√¥ h√¨nh t·ªëi ∆∞u cho b√†i to√°n ph√¢n lo·∫°i c·∫£m x√∫c.
        """)

        # Display Comparison Table
        st.markdown("### B·∫£ng so s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh:")
        comparison_data = {
            "Metric": ["Accuracy", "Precision (Class 0)", "Precision (Class 1)", "Recall (Class 0)", "Recall (Class 1)", "F1-Score (Class 0)", "F1-Score (Class 1)", "ROC-AUC Score"],
            "Logistic Regression": [0.9778, 0.63, 1.00, 0.96, 0.98, 0.76, 0.99, 0.99],
            "Multinomial Naive Bayes": [0.9639, 0.50, 1.00, 0.96, 0.96, 0.66, 0.98, 0.99],
            "XGBoost": [0.9849, 0.73, 1.00, 0.93, 0.99, 0.82, 0.99, 0.99],
            "SVM": [0.9776, 0.62, 1.00, 0.97, 0.98, 0.76, 0.99, 0.99]
        }

        df_comparison = pd.DataFrame(comparison_data)
        st.dataframe(df_comparison)

        # Recommendation Section
        st.markdown("### L·ª±a ch·ªçn m√¥ h√¨nh t·ªëi ∆∞u üìå")
        st.markdown("""
        #### L√Ω do l·ª±a ch·ªçn Logistic Regression:
        1. **D·ªÖ hi·ªÉu v√† gi·∫£i th√≠ch**:
            - Logistic Regression gi√∫p x√°c ƒë·ªãnh r√µ r√†ng t·∫ßm quan tr·ªçng c·ªßa t·ª´ng ƒë·∫∑c tr∆∞ng (t·ª´ kh√≥a) trong d·ª± ƒëo√°n.
            - Th√≠ch h·ª£p cho c√°c b√†i to√°n c·∫ßn minh b·∫°ch k·∫øt qu·∫£.

        2. **Hi·ªáu su·∫•t ·ªïn ƒë·ªãnh**:
            - ƒê·ªô ch√≠nh x√°c ƒë·∫°t **97.78%**, g·∫ßn t∆∞∆°ng ƒë∆∞∆°ng XGBoost.
            - F1-Score cho c·∫£ hai l·ªõp (T√≠ch c·ª±c, Ti√™u c·ª±c) r·∫•t t·ªët.

        3. **ƒê∆°n gi·∫£n v√† hi·ªáu qu·∫£**:
            - Y√™u c·∫ßu √≠t t√†i nguy√™n t√≠nh to√°n.
            - D·ªÖ d√†ng tri·ªÉn khai v√†o h·ªá th·ªëng th·ª±c t·∫ø.

        4. **Kh·∫£ nƒÉng m·ªü r·ªông**:
            - Th√≠ch h·ª£p v·ªõi d·ªØ li·ªáu l·ªõn v√† d·ªÖ d√†ng n√¢ng c·∫•p khi c√≥ th√™m d·ªØ li·ªáu m·ªõi.

        #### So s√°nh v·ªõi c√°c m√¥ h√¨nh kh√°c:
        - **XGBoost**: M·∫∑c d√π ch√≠nh x√°c h∆°n nh∆∞ng ph·ª©c t·∫°p v√† ƒë√≤i h·ªèi nhi·ªÅu t√†i nguy√™n.
        - **SVM**: C√¢n b·∫±ng hi·ªáu su·∫•t nh∆∞ng m·∫•t th·ªùi gian trong hu·∫•n luy·ªán khi d·ªØ li·ªáu l·ªõn.
        - **Multinomial Naive Bayes**: Nhanh nh∆∞ng hi·ªáu su·∫•t th·∫•p h∆°n Logistic Regression.
        """)

        # Step 8: System Deployment
        st.subheader("8. Tri·ªÉn khai h·ªá th·ªëng üöÄ")
        st.markdown("""
        ### M·ª•c ti√™u:
        Tri·ªÉn khai m√¥ h√¨nh h·ªçc m√°y v√†o h·ªá th·ªëng th·ª±c t·∫ø ƒë·ªÉ h·ªó tr·ª£ qu·∫£n l√Ω s·∫£n ph·∫©m v√† d·ªãch v·ª•.

        ### N·ªôi dung tri·ªÉn khai:
        1. **T√≠ch h·ª£p m√¥ h√¨nh tr√™n n·ªÅn t·∫£ng Hasaki.vn:**
            - Ph√¢n lo·∫°i ph·∫£n h·ªìi c·ªßa kh√°ch h√†ng theo th·ªùi gian th·ª±c.
            - Ph√¢n t√≠ch c·∫£m x√∫c ƒë·ªÉ cung c·∫•p th√¥ng tin h·ªØu √≠ch cho nh√† qu·∫£n l√Ω.
        2. **Hi·ªÉn th·ªã b√°o c√°o chi ti·∫øt:**
            - S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n t√≠ch c·ª±c/ti√™u c·ª±c.
            - Xu h∆∞·ªõng c·∫£m x√∫c theo s·∫£n ph·∫©m/d·ªãch v·ª•.
        """)

#####################################

elif page == "Sentiment Analysis":

    pkl_filemodel = "src/models/logreg_model.pkl" 
    with open(pkl_filemodel, 'rb') as file:  
        lgr_model_sentiment = pickle.load(file)

    # doc model count len
    pkl_count = "src/models/tfidf_vectorizer.pkl"  
    with open(pkl_count, 'rb') as file:  
        tfidf_vectorizer = pickle.load(file)

    plk_scaler = "src/models/minmax_scaler.pkl"
    with open(plk_scaler, 'rb') as file:
        scaler = pickle.load(file)

    pkl_svm='src/models/svm_model.pkl'
    with open(pkl_svm, 'rb') as file:  
        svm_model_sentiment = pickle.load(file)

    # Header
    st.title("üåü Ph√¢n T√≠ch Ph·∫£n H·ªìi üåü")

    # Introductory Text
    st.markdown("""
        üßñ‚Äç‚ôÄÔ∏è **M·ª•c ƒë√≠ch ch·ª©c nƒÉng**  
        - Ph√¢n t√≠ch ƒë√°nh gi√° c·ªßa kh√°ch h√†ng v·ªÅ c√°c s·∫£n ph·∫©m chƒÉm s√≥c da.  
        - Ph√°t hi·ªán nh·ªØng ph·∫£n h·ªìi thu·ªôc nh√≥m **Positive** üíñ ho·∫∑c **Negative** üíî.  
        - Gi√∫p b·∫°n ƒë∆∞a ra quy·∫øt ƒë·ªãnh d·ª±a tr√™n d·ªØ li·ªáu ƒë·ªÉ gi·ªØ cho kh√°ch h√†ng c·ªßa b·∫°n lu√¥n r·∫°ng r·ª°!  
    """)

    # Streamlit App
    st.subheader("üöÄ Ki·ªÉm tra n·ªôi dung b√¨nh lu·∫≠n m·ªõi")

    flag = False
    lines = None
    data_type = st.radio("Ch·ªçn h√¨nh th·ª©c g·ª≠i ph·∫£n h·ªìi", options=("üìù Nh·∫≠p t·ª´ b√†n ph√≠m", "üìÅ T·∫£i 1 file ph·∫£n h·ªìi"))
    
    # data_type = "üìù T·∫£i 1 file"
    if data_type == "üìÅ T·∫£i 1 file ph·∫£n h·ªìi":
        # Upload file
        uploaded_file = st.file_uploader("Vui l√≤ng ch·ªçn file t·∫£i l√™n (*.txt, *.csv):", type=['txt', 'csv'])
        
        # Add sample data link from data folder for download
        with open("src/data/sample_feedback.txt", "r") as file:
            sample_data = file.read()
        st.download_button(label="üì• File d·ªØ li·ªáu m·∫´u", data=sample_data, file_name="sample_feedback.txt", mime="text/plain")

        if uploaded_file is not None:
            try:
                # Read data
                if uploaded_file.name.endswith('.csv'):
                    lines = pd.read_csv(uploaded_file, header=None)
                else:
                    lines = pd.read_table(uploaded_file, header=None)
                
                st.write("üìÇ **D·ªØ li·ªáu ƒë√£ t·∫£i l√™n:**")
                lines.columns = ["content"]
                st.dataframe(lines)
                lines = lines["content"]
                flag = True
            except Exception as e:
                st.error(f"üö® Oops! Couldn‚Äôt read the file: {e}")

    # data_type = "üìù Nh·∫≠p t·ª´ b√†n ph√≠m"
    if data_type == "üìù Nh·∫≠p t·ª´ b√†n ph√≠m":
        content = st.text_area(label="N·ªôi dung ph·∫£n h·ªìi (c√≥ th·ªÉ nh·∫≠p nhi·ªÅu ph·∫£n h·ªìi khi 'Enter' xu·ªëng d√≤ng):", placeholder="e.g., S·∫£n ph·∫©m n√†y r·∫•t t·ªët!")
        if content != "":
            lines = content.split("\n")
            flag = True

    
    st.markdown(button_style, unsafe_allow_html=True)
    if st.button("Ph√¢n T√≠ch"):
        if flag:
            # st.subheader("üßê Processed Feedback")

            if len(lines) > 0:

                # Create a DataFrame with content as new reviews and column name as raw_content
                new_reviews = [str(line) for line in lines]
                df = pd.DataFrame(new_reviews, columns=['raw_content'])

                # Call clean_comment function (replace with your actual function implementation)
                df_new = content_process.clean_comment(df, 'raw_content', 'cleaned_content')
                
                # Display cleaned content
                # st.write(df_new['cleaned_content'])
                
                # Transform data using the vectorizer
                x_new = tfidf_vectorizer.transform(df_new['cleaned_content'])

                # Create a DataFrame for the new reviews
                df_new_review = pd.DataFrame(x_new.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

                # Add and scale the 'content_length' feature like you did during training
                df_new_review['content_length'] = [len(review) for review in new_reviews]
                df_new_review['content_length_scaled'] = scaler.transform(df_new_review[['content_length']]) # Use the same scaler from training

                # Combine features
                new_reviews_combined = sp.hstack((x_new, df_new_review[['content_length_scaled']]))

                # Predict sentiment by Logistic 
                # y_pred_new = lgr_model_sentiment.predict(new_reviews_combined)

                # Predict sentiment by svm 
                y_pred_new = svm_model_sentiment.predict(new_reviews_combined)
                
                # Map predictions to sentiment labels
                sentiment_labels = {0: "üíî Negative", 1: "üíñ Positive"}
                predictions = [sentiment_labels[pred] for pred in y_pred_new]
                
                # Display predictions
                st.subheader("üéØ K·∫øt qu·∫£ ph√¢n t√≠ch:")

                # NEW VERSION
                df = pd.DataFrame(
                {
                    "N·ªôi Dung": lines,
                    "Sentiment": predictions,
                })

                # Apply custom CSS for alternating row colors based on sentiment
                def color_row(row):
                    if row['Sentiment'] == "üíñ Positive":
                        return ['background-color: #ebf8ee'] * len(row)
                    else:
                        return ['background-color: #ffedec'] * len(row)

                styled_df = df.style.apply(color_row, axis=1)

                st.markdown(styled_df.to_html(), unsafe_allow_html=True)

#####################################

elif page == "Product Analysis":
    st.title("üåü Ph√¢n T√≠ch S·∫£n Ph·∫©m üåü")
    st.write("D·ª±a v√†o k·∫øt qu·∫£ ph√¢n t√≠ch, Hasaki v√† c√°c ƒë·ªëi t√°c s·∫Ω hi·ªÉu ƒë∆∞·ª£c c·∫£m nh·∫≠n c·ªßa kh√°ch h√†ng v·ªÅ s·∫£n ph·∫©m.")
    
    df_products = helper.read_csv("src/data/San_pham.csv")
    df_raw_reviews = helper.read_csv("src/data/Danh_gia.csv")
    df_merged = pd.merge(df_raw_reviews, df_products, on="ma_san_pham", how="inner")

    # Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu (M√£ s·∫£n ph·∫©m ho·∫∑c T√™n s·∫£n ph·∫©m)
    input_method = st.radio("Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu:", ["Ch·ªçn t√™n s·∫£n ph·∫©m", "Nh·∫≠p m√£/t√™n s·∫£n ph·∫©m"])

    product_code = None
    if input_method == "Nh·∫≠p m√£/t√™n s·∫£n ph·∫©m":
        # Nh·∫≠p m√£ s·∫£n ph·∫©m ho·∫∑c t√™n s·∫£n ph·∫©m
        search_criteria = st.text_input("Nh·∫≠p m√£ ho·∫∑c t√™n ƒë·∫ßy ƒë·ªß c·ªßa s·∫£n ph·∫©m:")
        
        # Add sample text for product name and product code
        st.markdown("üìù **V√≠ d·ª• t√™n s·∫£n ph·∫©m:** N∆∞·ªõc Hoa H·ªìng, Kem Ch·ªëng N·∫Øng  \nüìù **V√≠ d·ª• m√£ s·∫£n ph·∫©m:** 318900012")
        
        if (search_criteria != "" and search_criteria.isdigit()):
            result = df_products[df_products["ma_san_pham"] == eval(search_criteria)]
            if not result.empty:
                product_code = result["ma_san_pham"].iloc[0]
            else:
                st.write("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m!")
            
        elif (search_criteria != ""):
            result = df_products[df_products["ten_san_pham"].str.contains(search_criteria)]
            if not result.empty:
                if "df_found" not in st.session_state:
                    st.session_state.df_found = pd.DataFrame(
                        result[['ma_san_pham', 'ten_san_pham']].drop_duplicates(), columns=["ma_san_pham", "ten_san_pham"])
                st.write("üîç **K·∫øt qu·∫£ t√¨m ki·∫øm:**")
                event = st.dataframe(
                    st.session_state.df_found,
                    key="data",
                    on_select="rerun",
                    selection_mode=["single-row"])
                
                if len(event.selection.rows) > 0:
                    idx = event.selection.rows[0]
                    selected_row = st.session_state.df_found.iloc[int(idx)]
                    product_code = selected_row["ma_san_pham"]
                else:
                    selected_row = st.session_state.df_found.iloc[0] # Default get first row
                    product_code = selected_row["ma_san_pham"]
            else:
                st.write("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m!")
        
    else:   
        # Ch·ªçn t√™n s·∫£n ph·∫©m t·ª´ dropdown
        selected_item = st.selectbox("Ch·ªçn t√™n s·∫£n ph·∫©m:", df_merged['ten_san_pham'].unique())
        product_code = df_products[df_products["ten_san_pham"] == selected_item]["ma_san_pham"].iloc[0]

    # Hi·ªÉn th·ªã th√¥ng tin s·∫£n ph·∫©m
    st.markdown(button_style, unsafe_allow_html=True)
    if st.button("Ph√¢n T√≠ch"):
        if not product_code:
            st.error("M√£ s·∫£n ph·∫©m kh√¥ng t·ªìn t·∫°i trong c∆° s·ªü d·ªØ li·ªáu. Vui l√≤ng ch·ªçn ho·∫∑c nh·∫≠p m√£ s·∫£n ph·∫©m h·ª£p l·ªá.")
        else:
            df_review = helper.read_csv("src/data/step2_full_review_result.csv")
            st.subheader("K·∫øt qu·∫£ ph√¢n t√≠ch s·∫£n ph·∫©m")
            
            st.markdown("#### 1. Th√¥ng tin s·∫£n ph·∫©m:")
            product_analysis.GetProductInfoByCode(df_merged, product_code)

            st.markdown("#### 2. Th√¥ng tin nh·∫≠n x√©t:")
            product_analysis.GetProductReview(df_review, product_code, df_raw_reviews)
            